{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMJYfysaREkb"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mdEmY4rDQ3ik",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ast import literal_eval\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Scienfitic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from torch import cuda\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Visuals\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context=\"notebook\",\n",
    "        rc={\"font.size\":16,\n",
    "            \"axes.titlesize\":16,\n",
    "            \"axes.labelsize\":16,\n",
    "            \"xtick.labelsize\": 16.0,\n",
    "            \"ytick.labelsize\": 16.0,\n",
    "            \"legend.fontsize\": 16.0})\n",
    "palette_ = sns.color_palette(\"Set1\")\n",
    "palette = palette_[2:5] + palette_[7:]\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# Utilities\n",
    "\n",
    "from general_utils import (\n",
    "  ModelAndTokenizer,\n",
    "  make_inputs,\n",
    "  decode_tokens,\n",
    "  find_token_range,\n",
    "  predict_from_input,\n",
    ")\n",
    "\n",
    "from patchscopes_utils import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_hook = {\n",
    "    \"EleutherAI/pythia-6.9b\": set_hs_patch_hooks_neox,\n",
    "    \"EleutherAI/pythia-12b\": set_hs_patch_hooks_neox,\n",
    "    \"meta-llama/Llama-2-13b-hf\": set_hs_patch_hooks_llama,\n",
    "    \"lmsys/vicuna-7b-v1.5\": set_hs_patch_hooks_llama,\n",
    "    \"./stable-vicuna-13b\": set_hs_patch_hooks_llama,\n",
    "    \"CarperAI/stable-vicuna-13b-delta\": set_hs_patch_hooks_llama,\n",
    "    \"EleutherAI/gpt-j-6b\": set_hs_patch_hooks_gptj\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "4479f16b9a544b79bb8790693701d8de"
     ]
    },
    "id": "fKGGJO3GQ3in",
    "outputId": "aed82adb-d542-4de6-ade7-c2a4f7aadcc6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08923222f8844c7a9912d848fd92ca80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50432, 4096)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=4096, out_features=50432, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model 1\n",
    "\n",
    "model_name_1 = \"EleutherAI/pythia-6.9b\"\n",
    "sos_tok_1 = False\n",
    "\n",
    "if \"13b\" in model_name_1 or \"12b\" in model_name_1:\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    torch_dtype = None\n",
    "\n",
    "mt_1 = ModelAndTokenizer(\n",
    "    model_name_1,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "mt_1.set_hs_patch_hooks = model_to_hook[model_name_1]\n",
    "mt_1.model.eval()\n",
    "mt_1.model.to(mt_1.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "mt_2 = mt_1\n",
    "sos_tok_2 = sos_tok_1\n",
    "model_name_2 = model_name_1\n",
    "model_name_1_ = model_name_1.strip('./')\n",
    "model_name_2_ = model_name_1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37713c896324949a78091dcbf879fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "pile_dataset = load_dataset(\"monology/pile-uncopyrighted\", streaming=True, split='train')\n",
    "pile_dataset = pile_dataset.shuffle(seed=42)\n",
    "\n",
    "trn_n = 200 # small dataset for testing\n",
    "val_n = 10\n",
    "pile_trn = []\n",
    "pile_val = []\n",
    "\n",
    "# Take first trn_n examples for training\n",
    "for i, example in enumerate(pile_dataset):\n",
    "    if i < trn_n:\n",
    "        pile_trn.append(example['text'])\n",
    "    elif i < trn_n + val_n:\n",
    "        pile_val.append(example['text'])\n",
    "    else:\n",
    "        break\n",
    "\n",
    "sentences = [(x, 'train') for x in pile_trn] + [(x, 'validation') for x in pile_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:13<00:00, 15.68it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 256\n",
    "\n",
    "data = {}\n",
    "for sentence, split in tqdm(sentences):\n",
    "    \n",
    "    inp_1_ = make_inputs(mt_1.tokenizer, [sentence], device=mt_1.device)\n",
    "    inp_2_ = make_inputs(mt_2.tokenizer, [sentence], device=mt_2.device)\n",
    "    position = None\n",
    "    k = 0\n",
    "    while k<10:\n",
    "        position_tmp = random.randint(\n",
    "            0, min(max_len - 1, \n",
    "                   len(inp_1_['input_ids'][0]) - 1, \n",
    "                   len(inp_2_['input_ids'][0]) - 1)\n",
    "        )\n",
    "        # cut the tokenized input at the sampled position and turn it back into a string.\n",
    "        # add some buffer at the end such that the tokenization is not modified around the sampled position.\n",
    "        prefix_1 = mt_1.tokenizer.decode(inp_1_['input_ids'][0][:position_tmp + int(sos_tok_1) + 5])\n",
    "        prefix_2 = mt_2.tokenizer.decode(inp_2_['input_ids'][0][:position_tmp + int(sos_tok_2) + 5])\n",
    "        \n",
    "        # check that the selected position corresponds to the same part of the string by \n",
    "        # comparing the prefixes until the sampled position. also make sure that this re-tokenization\n",
    "        # does not shift the sampled position off the sequence length.\n",
    "        inp_1 = make_inputs(mt_1.tokenizer, [prefix_1], device=mt_1.device)\n",
    "        inp_2 = make_inputs(mt_2.tokenizer, [prefix_2], device=mt_2.device)\n",
    "        if prefix_1 == prefix_2 and position_tmp < min(len(inp_1['input_ids'][0]), \n",
    "                                                       len(inp_2['input_ids'][0])):\n",
    "            position = position_tmp\n",
    "            break\n",
    "        k += 1\n",
    "    if position is None:\n",
    "        continue\n",
    "    \n",
    "    for mt, model_name, inp, sos_tok in zip(\n",
    "        [mt_1, mt_2],\n",
    "        [model_name_1, model_name_2],\n",
    "        [inp_1, inp_2],\n",
    "        [sos_tok_1, sos_tok_2]\n",
    "    ):\n",
    "        position_ = position + int(sos_tok)\n",
    "        if (prefix_1, position_, split, model_name) not in data:\n",
    "            output = mt.model(**inp, output_hidden_states = True)\n",
    "\n",
    "            data[(prefix_1, position_, split, model_name)] =  [\n",
    "                output[\"hidden_states\"][layer+1][0][position_].detach().float().cpu().numpy()\n",
    "                for layer in range(mt.num_layers)\n",
    "            ]\n",
    "\n",
    "df = pd.Series(data).reset_index()\n",
    "df.columns = ['full_text', 'position', 'data_split', 'model_name', 'hidden_rep']   \n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "for model_name in [model_name_1, model_name_2]:\n",
    "    # Create model-specific output directory\n",
    "    os.makedirs(os.path.dirname(f\"outputs/{model_name}_pile_trn_val.pkl\"), exist_ok=True)\n",
    "    df[df['model_name'] == model_name].to_pickle(f\"outputs/{model_name}_pile_trn_val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad and unpad \n",
    "\n",
    "pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])\n",
    "unpad = lambda x: x[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sources = [l for l in range(0, mt_1.num_layers, 5)]\n",
    "layer_targets = [l for l in range(0, mt_2.num_layers, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]/tmp/ipykernel_416/3176932371.py:27: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  A, res, rank, s = np.linalg.lstsq(pad(X), pad(Y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 max error on train: 2.6645352591003757e-13\n",
      "0 5 max error on train: 1.6484591469634324e-12\n",
      "0 10 max error on train: 2.0463630789890885e-12\n",
      "0 15 max error on train: 2.3447910280083306e-12\n",
      "0 20 max error on train: 2.8137492336099967e-12\n",
      "0 25 max error on train: 3.666400516522117e-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:21<02:09, 21.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30 max error on train: 3.211653165635653e-12\n",
      "5 0 max error on train: 4.263256414560601e-13\n",
      "5 5 max error on train: 2.2737367544323206e-12\n",
      "5 10 max error on train: 1.4068746168049984e-12\n",
      "5 15 max error on train: 2.7284841053187847e-12\n",
      "5 20 max error on train: 5.4569682106375694e-12\n",
      "5 25 max error on train: 6.366462912410498e-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:41<01:43, 20.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 30 max error on train: 8.412825991399586e-12\n",
      "10 0 max error on train: 3.936406756110955e-12\n",
      "10 5 max error on train: 3.751665644813329e-12\n",
      "10 10 max error on train: 3.410605131648481e-12\n",
      "10 15 max error on train: 5.002220859751105e-12\n",
      "10 20 max error on train: 2.1373125491663814e-11\n",
      "10 25 max error on train: 2.000888343900442e-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [01:01<01:20, 20.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 30 max error on train: 2.205524651799351e-11\n",
      "15 0 max error on train: 9.805489753489383e-13\n",
      "15 5 max error on train: 1.9326762412674725e-12\n",
      "15 10 max error on train: 2.5011104298755527e-12\n",
      "15 15 max error on train: 2.7853275241795927e-12\n",
      "15 20 max error on train: 8.640199666842818e-12\n",
      "15 25 max error on train: 1.000444171950221e-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [01:22<01:02, 20.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 30 max error on train: 5.6843418860808015e-12\n",
      "20 0 max error on train: 5.542233338928781e-13\n",
      "20 5 max error on train: 1.7053025658242404e-12\n",
      "20 10 max error on train: 2.2737367544323206e-12\n",
      "20 15 max error on train: 1.8189894035458565e-12\n",
      "20 20 max error on train: 1.5063505998114124e-12\n",
      "20 25 max error on train: 3.183231456205249e-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [01:44<00:42, 21.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 30 max error on train: 2.0889956431346945e-12\n",
      "25 0 max error on train: 6.536993168992922e-13\n",
      "25 5 max error on train: 4.774847184307873e-12\n",
      "25 10 max error on train: 5.002220859751105e-12\n",
      "25 15 max error on train: 4.774847184307873e-12\n",
      "25 20 max error on train: 2.7284841053187847e-12\n",
      "25 25 max error on train: 1.9326762412674725e-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [02:05<00:21, 21.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 30 max error on train: 3.467448550509289e-12\n",
      "30 0 max error on train: 9.094947017729282e-13\n",
      "30 5 max error on train: 4.774847184307873e-12\n",
      "30 10 max error on train: 4.092726157978177e-12\n",
      "30 15 max error on train: 3.637978807091713e-12\n",
      "30 20 max error on train: 2.7853275241795927e-12\n",
      "30 25 max error on train: 2.9558577807620168e-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:26<00:00, 20.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30 max error on train: 2.0463630789890885e-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/root/interpretability/patchscopes/code/EleutherAI/pythia-6.9b_EleutherAI/pythia-6.9b_mappings_pile.zip'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = f'{model_name_1}_{model_name_2_}_mappings_pile'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "df_trn_1 = pd.DataFrame(df[(df['data_split'] == 'train') & \n",
    "                           (df['model_name'] == model_name_1)]['hidden_rep'].to_list(), \n",
    "                        columns=[layer for layer in range(mt_1.num_layers)])\n",
    "df_trn_2 = pd.DataFrame(df[(df['data_split'] == 'train') & \n",
    "                           (df['model_name'] == model_name_2)]['hidden_rep'].to_list(), \n",
    "                        columns=[layer for layer in range(mt_2.num_layers)])\n",
    "\n",
    "layer_sources = [l for l in range(0, mt_1.num_layers, 5)]\n",
    "layer_targets = [l for l in range(0, mt_2.num_layers, 5)]\n",
    "\n",
    "mappings = {}\n",
    "for layer_source in tqdm(layer_sources):\n",
    "    for layer_target in layer_targets:\n",
    "        X = np.array(\n",
    "            df_trn_1[layer_source].values.tolist()\n",
    "        )\n",
    "        Y = np.array(\n",
    "            df_trn_2[layer_target].values.tolist()\n",
    "        )\n",
    "\n",
    "        # Solve the least squares problem X * A = Y\n",
    "        # to find our transformation matrix A\n",
    "        A, res, rank, s = np.linalg.lstsq(pad(X), pad(Y))\n",
    "        transform = lambda x: unpad(pad(x) @ A)\n",
    "\n",
    "        mappings[(layer_source, layer_target)] = A\n",
    "        with open(f'{model_name_1}_{model_name_2_}_mappings_pile/mapping_{layer_source}-{layer_target}.npy', 'wb') as fd:\n",
    "            np.save(fd, A)\n",
    "\n",
    "        print(layer_source, layer_target, \"max error on train:\", np.abs(Y - transform(X)).max())\n",
    "\n",
    "shutil.make_archive(output_dir, 'zip', output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "mappings = {}\n",
    "for layer_source in tqdm(layer_sources):\n",
    "    for layer_target in layer_targets:\n",
    "        with open(f'{model_name_1}_{model_name_2_}_mappings_pile/mapping_{layer_source}-{layer_target}.npy', 'rb') as fd:\n",
    "            A = np.load(fd)\n",
    "        mappings[(layer_source, layer_target)] = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-organize validation set\n",
    "\n",
    "# First get validation data and group by text\n",
    "df_val = df[(df['data_split'] == 'validation')]\n",
    "\n",
    "# Split into model 1 and model 2 data\n",
    "df_val_1 = df_val[df_val['model_name'] == model_name_1]\n",
    "df_val_2 = df_val[df_val['model_name'] == model_name_2]\n",
    "\n",
    "# Merge the two dataframes\n",
    "df_val = pd.merge(\n",
    "    df_val_1[['full_text', 'position', 'model_name', 'hidden_rep']],\n",
    "    df_val_2[['full_text', 'position', 'model_name', 'hidden_rep']], \n",
    "    on='full_text',\n",
    "    suffixes=('_1', '_2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:14<00:00,  2.07s/it]\n",
      "100%|██████████| 7/7 [00:13<00:00,  2.00s/it]\n",
      "100%|██████████| 7/7 [00:13<00:00,  2.00s/it]\n",
      "100%|██████████| 7/7 [00:14<00:00,  2.04s/it]\n",
      "100%|██████████| 7/7 [00:14<00:00,  2.01s/it]\n",
      "100%|██████████| 7/7 [00:14<00:00,  2.00s/it]\n",
      " 86%|████████▌ | 6/7 [01:24<00:14, 14.10s/it]"
     ]
    }
   ],
   "source": [
    "# Evaluate linear mappings on the validation set of WikiText/a sample from the Pile\n",
    "\n",
    "records = []\n",
    "for layer_source in tqdm(layer_sources):\n",
    "    for layer_target in tqdm(layer_targets):\n",
    "        A = mappings[(layer_source, layer_target)]\n",
    "        transform = lambda x: torch.tensor(\n",
    "            np.squeeze(\n",
    "                unpad(np.dot(\n",
    "                    pad(np.expand_dims(x.detach().float().cpu().numpy(), 0)), \n",
    "                    A\n",
    "                ))\n",
    "            )\n",
    "        ).to(mt_2.device)\n",
    "\n",
    "        for idx, row in df_val.iterrows():\n",
    "            prompt = row['full_text']\n",
    "            position_source = row['position_1']\n",
    "            position_target = row['position_2']\n",
    "            prec_1, surprisal = evaluate_patch_next_token_prediction_x_model(\n",
    "                mt_1, mt_2, prompt, prompt, layer_source, layer_target,\n",
    "                position_source, position_target, position_prediction=position_target, transform=transform)\n",
    "\n",
    "            records.append({'layer_source': layer_source,\n",
    "                            'layer_target': layer_target,\n",
    "                            'position_source': position_source,\n",
    "                            'position_target': position_target,\n",
    "                            'prec_1': prec_1, \n",
    "                            'surprisal': surprisal})\n",
    "        \n",
    "\n",
    "results = pd.DataFrame.from_records(records)\n",
    "results.to_csv(f'{model_name_1}_{model_name_2_}_mappings_pile_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulted heatmap\n",
    "metric = 'prec_1'\n",
    "tmp = results[['layer_source', 'layer_target', metric]].groupby(['layer_source', 'layer_target']).agg(\"mean\").reset_index()\n",
    "tmp = tmp.pivot(index='layer_source', columns='layer_target', values=metric)\n",
    "\n",
    "sns.heatmap(tmp, annot=True, fmt=\".1f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate identity mapping on the validation set of WikiText\n",
    "\n",
    "records = []\n",
    "for layer_source in tqdm(layer_sources):\n",
    "    for layer_target in tqdm(layer_targets):\n",
    "        for idx, row in df_val.iterrows():\n",
    "            prompt = row['full_text']\n",
    "            position_source = row['position_1']\n",
    "            position_target = row['position_2']\n",
    "            prec_1, surprisal = evaluate_patch_next_token_prediction_x_model(\n",
    "                mt_1, mt_2, prompt, prompt, layer_source, layer_target,\n",
    "                position_source, position_target, position_prediction=position_target)\n",
    "\n",
    "            records.append({'layer_source': layer_source,\n",
    "                            'layer_target': layer_target,\n",
    "                            'position_source': position_source,\n",
    "                            'position_target': position_target,\n",
    "                            'prec_1': prec_1, \n",
    "                            'surprisal': surprisal})\n",
    "        \n",
    "results = pd.DataFrame.from_records(records)\n",
    "results.to_csv(f'{model_name_1}_{model_name_2_}_identity_pile_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the ID prompt on the validation set of WikiText\n",
    "\n",
    "prompt_target = \"cat -> cat\\n1135 -> 1135\\nhello -> hello\\n?\"\n",
    "position_target = -1\n",
    "\n",
    "records = []\n",
    "for layer_source in tqdm(layer_sources):\n",
    "    for layer_target in tqdm(layer_targets):\n",
    "        for idx, row in df_val.iterrows():\n",
    "            prompt_source = row['full_text']\n",
    "            position_source = row['position_1']\n",
    "            prec_1, surprisal = evaluate_patch_next_token_prediction_x_model(\n",
    "                mt_1, mt_2, prompt_source, prompt_target, layer_source, layer_target,\n",
    "                position_source, position_target, position_prediction=position_target, transform=None)\n",
    "\n",
    "            records.append({'layer_source': layer_source,\n",
    "                            'layer_target': layer_target,\n",
    "                            'position_source': position_source,\n",
    "                            'position_target': position_target,\n",
    "                            'prec_1': prec_1, \n",
    "                            'surprisal': surprisal})\n",
    "        \n",
    "results = pd.DataFrame.from_records(records)\n",
    "results.to_csv(f'{model_name_1}_{model_name_2_}_prompt-id_pile_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = pd.read_csv(f'{model_name_1}_{model_name_2_}_identity_pile_eval.csv')\n",
    "results1[\"variant\"] = \"identity\"\n",
    "results2 = pd.read_csv(f'{model_name_1}_{model_name_2_}_mappings_pile_eval.csv')\n",
    "results2[\"variant\"] = \"affine mapping\"\n",
    "results3 = pd.read_csv(f'{model_name_1}_{model_name_2_}_prompt-id_pile_eval.csv')\n",
    "results3[\"variant\"] = \"prompt id\"\n",
    "\n",
    "results = pd.concat([results1, results2, results3], ignore_index=True)\n",
    "\n",
    "for metric in ['prec_1', 'surprisal']:\n",
    "    ax = sns.lineplot(data=results, x='layer', y=metric, hue=\"variant\")\n",
    "    ax.set_title(f\"{model_name_1.strip('./')} --> {model_name_2_}\")\n",
    "    ax.legend_.set_title('')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
